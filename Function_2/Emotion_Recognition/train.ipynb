{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (224, 224)\n",
    "BATCH_SIZE = 80\n",
    "INPUT_SIZE = (224, 224)\n",
    "\n",
    "# Define FER2013 dataset paths\n",
    "train_data_dir = 'fer2013/train'\n",
    "val_data_dir = 'fer2013/val'\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=None, horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=None)\n",
    "\n",
    "# Load data (change color_mode to 'rgb' since you're converting to RGB in preprocessing)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, target_size=INPUT_SIZE,\n",
    "    batch_size=BATCH_SIZE, class_mode='categorical', color_mode='rgb'\n",
    ")\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "    val_data_dir, target_size=INPUT_SIZE,\n",
    "    batch_size=BATCH_SIZE, class_mode='categorical', color_mode='rgb'\n",
    ")\n",
    "\n",
    "# Get one batch of images from the generator\n",
    "x_batch, y_batch = next(train_generator)\n",
    "\n",
    "# Check the shape of the first image in the batch\n",
    "print(x_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 224, 224, 3)\n",
      "Found 28474 images belonging to 7 classes.\n",
      "Found 7022 images belonging to 7 classes.\n",
      "(80, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "base_model = load_model('mobilenet_face.h5') # face detecton model\n",
    "print(base_model.input_shape)\n",
    "net_description = 'mobilenet_face'\n",
    "\n",
    "def preprocess_fer2013(image):\n",
    "    # Convert grayscale image to RGB by repeating the single channel across 3 channels\n",
    "    image = np.repeat(image[..., np.newaxis], 3, axis=-1)  # Correctly expand to (224, 224, 3)\n",
    "    return image\n",
    "\n",
    "preprocessing_function = preprocess_fer2013\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 28474 7022\n",
      "{0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
      "[3995  436 4097 7214 4965 4596 3171] {0: 1.8057571964956194, 1: 16.545871559633028, 2: 1.7608005857944837, 3: 1.0, 4: 1.4529707955689828, 5: 1.5696257615317666, 6: 2.2749921160517186} {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'} {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
      "mobilenet_face\n",
      "(None, 224, 224, 3)\n",
      "Epoch 1/3\n",
      "355/355 [==============================] - ETA: 0s - loss: 2.4509 - accuracy: 0.5033\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56293, saving model to mobilenet_face.h5\n",
      "355/355 [==============================] - 199s 556ms/step - loss: 2.4509 - accuracy: 0.5033 - val_loss: 1.1720 - val_accuracy: 0.5629\n",
      "Epoch 2/3\n",
      "355/355 [==============================] - ETA: 0s - loss: 2.0897 - accuracy: 0.5676\n",
      "Epoch 2: val_accuracy improved from 0.56293 to 0.57328, saving model to mobilenet_face.h5\n",
      "355/355 [==============================] - 27s 75ms/step - loss: 2.0897 - accuracy: 0.5676 - val_loss: 1.1529 - val_accuracy: 0.5733\n",
      "Epoch 3/3\n",
      "355/355 [==============================] - ETA: 0s - loss: 2.0410 - accuracy: 0.5776\n",
      "Epoch 3: val_accuracy improved from 0.57328 to 0.57414, saving model to mobilenet_face.h5\n",
      "355/355 [==============================] - 27s 76ms/step - loss: 2.0410 - accuracy: 0.5776 - val_loss: 1.1502 - val_accuracy: 0.5741\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_CLASS=val_generator.num_classes\n",
    "nb_train_samples=train_generator.samples\n",
    "nb_validation_samples=val_generator.samples\n",
    "print(N_CLASS,nb_train_samples,nb_validation_samples)\n",
    "\n",
    "class_to_idx=val_generator.class_indices\n",
    "idx_to_class={class_to_idx[cls]:cls for cls in class_to_idx}\n",
    "print(idx_to_class)\n",
    "\n",
    "(unique, counts) = np.unique(train_generator.classes, return_counts=True)\n",
    "cw=1/counts\n",
    "cw/=cw.min()\n",
    "class_weights = {i:cwi for i,cwi in zip(unique,cw)}\n",
    "print(counts, class_weights, idx_to_class, val_generator.class_indices)\n",
    "\n",
    "layer_name='feats'\n",
    "#layer_name='global_average_pooling2d_1'\n",
    "#layer_name='fc7/relu'\n",
    "layer_out=base_model.get_layer(layer_name) #'global_pooling') #\n",
    "x=layer_out.output\n",
    "\n",
    "emotion_preds = Dense(N_CLASS, activation='softmax', name='emotion_preds')(x)\n",
    "model=Model(base_model.input,emotion_preds)\n",
    "start_epoch=0\n",
    "\n",
    "base_model.trainable=False\n",
    "for l in base_model.layers:\n",
    "    l.trainable=False\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(net_description)\n",
    "\n",
    "mc = ModelCheckpoint(net_description+'.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "es=EarlyStopping(monitor='val_accuracy',patience=2)\n",
    "FIRST_EPOCHS=3\n",
    "\n",
    "print(model.input_shape)\n",
    "\n",
    "hist1=model.fit(train_generator, steps_per_epoch=nb_train_samples//BATCH_SIZE, epochs=FIRST_EPOCHS, verbose=1, \n",
    "                    initial_epoch=0, callbacks=[mc, es], validation_data=val_generator, validation_steps=nb_validation_samples // BATCH_SIZE,class_weight=class_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining the model for more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.7821 - accuracy: 0.6239\n",
      "Epoch 4: val_accuracy improved from -inf to 0.65014, saving model to mobilenet_face_ft.h5\n",
      "355/355 [==============================] - 150s 416ms/step - loss: 1.7821 - accuracy: 0.6239 - val_loss: 0.9601 - val_accuracy: 0.6501\n",
      "Epoch 5/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.5615 - accuracy: 0.6648\n",
      "Epoch 5: val_accuracy improved from 0.65014 to 0.65761, saving model to mobilenet_face_ft.h5\n",
      "355/355 [==============================] - 144s 405ms/step - loss: 1.5615 - accuracy: 0.6648 - val_loss: 0.9589 - val_accuracy: 0.6576\n",
      "Epoch 6/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.4413 - accuracy: 0.6838\n",
      "Epoch 6: val_accuracy improved from 0.65761 to 0.67213, saving model to mobilenet_face_ft.h5\n",
      "355/355 [==============================] - 136s 382ms/step - loss: 1.4413 - accuracy: 0.6838 - val_loss: 0.9112 - val_accuracy: 0.6721\n",
      "Epoch 7/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.3121 - accuracy: 0.7072\n",
      "Epoch 7: val_accuracy improved from 0.67213 to 0.68333, saving model to mobilenet_face_ft.h5\n",
      "355/355 [==============================] - 118s 333ms/step - loss: 1.3121 - accuracy: 0.7072 - val_loss: 0.8965 - val_accuracy: 0.6833\n",
      "Epoch 8/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.2193 - accuracy: 0.7267\n",
      "Epoch 8: val_accuracy did not improve from 0.68333\n",
      "355/355 [==============================] - 106s 298ms/step - loss: 1.2193 - accuracy: 0.7267 - val_loss: 0.9330 - val_accuracy: 0.6757\n",
      "Epoch 9/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.1404 - accuracy: 0.7458\n",
      "Epoch 9: val_accuracy improved from 0.68333 to 0.69181, saving model to mobilenet_face_ft.h5\n",
      "355/355 [==============================] - 84s 237ms/step - loss: 1.1404 - accuracy: 0.7458 - val_loss: 0.8874 - val_accuracy: 0.6918\n",
      "Epoch 10/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 1.0545 - accuracy: 0.7603\n",
      "Epoch 10: val_accuracy did not improve from 0.69181\n",
      "355/355 [==============================] - 102s 286ms/step - loss: 1.0545 - accuracy: 0.7603 - val_loss: 0.8941 - val_accuracy: 0.6905\n",
      "Epoch 11/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 0.9783 - accuracy: 0.7802\n",
      "Epoch 11: val_accuracy improved from 0.69181 to 0.69713, saving model to mobilenet_face_ft.h5\n",
      "355/355 [==============================] - 85s 240ms/step - loss: 0.9783 - accuracy: 0.7802 - val_loss: 0.8880 - val_accuracy: 0.6971\n",
      "Epoch 12/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 0.8979 - accuracy: 0.7978\n",
      "Epoch 12: val_accuracy did not improve from 0.69713\n",
      "355/355 [==============================] - 78s 221ms/step - loss: 0.8979 - accuracy: 0.7978 - val_loss: 0.9163 - val_accuracy: 0.6921\n",
      "Epoch 13/13\n",
      "355/355 [==============================] - ETA: 0s - loss: 0.8232 - accuracy: 0.8162\n",
      "Epoch 13: val_accuracy did not improve from 0.69713\n",
      "355/355 [==============================] - 81s 228ms/step - loss: 0.8232 - accuracy: 0.8162 - val_loss: 0.9123 - val_accuracy: 0.6931\n"
     ]
    }
   ],
   "source": [
    "start_epoch=len(hist1.history['loss'])\n",
    "for l in base_model.layers:\n",
    "    l.trainable=True\n",
    "    \n",
    "model.load_weights(net_description+'.h5')\n",
    "model.compile(optimizer=Adam(lr=1e-4,decay=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "SECOND_EPOCHS=start_epoch+10\n",
    "mc = ModelCheckpoint(net_description+'_ft.h5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "#es=EarlyStopping(monitor='val_accuracy',patience=2 )\n",
    "\n",
    "hist2=model.fit(train_generator, steps_per_epoch=train_generator.samples//BATCH_SIZE, epochs=SECOND_EPOCHS, verbose=1, \n",
    "                    initial_epoch=start_epoch, validation_data=val_generator, validation_steps=val_generator.samples // BATCH_SIZE, callbacks=[mc],class_weight=class_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
